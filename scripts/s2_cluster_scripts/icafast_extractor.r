### icafast_extractor.r / icafast_extractor_job.sh are paired R script and sbatch script for the cluster

### INPUTS are 1. lists of icafast outputs (generated by icafast_50.r,...icafast_250.r / icafast_50_job.sh,...icafast_250_job.sh;
### and null_icafast_all.r / null_icafast_all_job.sh cluster scripts); and 2. the original data matrix provided to icafast

### OUTPUTS saved are 1. consensus (_c) and "diagonal" (_d) S and A matrices from ica runs, consistently reoriented and ordered 
### by their "variance accounted for"; 2. Spearman correlation matrices and PAM clustering outputs for S components (comp. 
### demanding steps for consensus extraction); 3. Silhouette scores (global scores per ncomp, and those per consensus component)
### extracted from PAM output for convenience; 4. Reconstitution errors per ncomp; 5. "Variance accounted for" for consensus and
### "diagonal" components, and null vafs (from ica runs on permuted data matrices from null_icafast_all.r)

### this performed for each of the investigated number of ica components (ncomp, in this case 50,100,150,200,250)

### this is an instance of script (e.g. for the spikeF or sumF version of normalised data, see dataprep.r script)
### time-permitting, it can be made into a function if needs to be run on multiple versions of normalised data..

library(cluster)

data.version <- "spikeF" ### CHANGE ACCORDINGLY ! ###  ("sumF" or other...)

#-------------------------------------------------------------------------------------------------------------------------------#
if(data.version == "spikeF") {
  
  print("data version: spikeF"); cat("\n")
  
  silh_width_ave <- rep(0,5)
  silh_width_clust <- list()
  
  S_c <- list()
  A_c <- list()
  
  S_d <- list()
  A_d <- list()
  
  norm_F_error_c <- rep(0,5)
  norm_F_error_d <- rep(0,5)
  
  vaf_c <- list()
  vaf_d <- list()
  null_vaf <- list()
  
  # read-in the original data matrix which was input for ica
  X <- readRDS(file = "/data/cephfs/punim0613/icafast/data/tm.smartseq2.spikeF.mat.hvg.123.rds")
  X <- as.matrix(X)
  X <- scale(X, scale = FALSE)
  # prep this to calculate relative Frobenius errors & "variance accounted for" below
  F_X2 <- (norm(X, type = "F"))^2 # squared Frobenius norm of data matrix
  ngenes <- nrow(X)
  
  # read-in fastica output lists for each ncomp
  for(ncomp in c(50,100,150,200,250)) {
    
    
    j <- ncomp / 50
    
    connect <- paste0("/data/cephfs/punim0613/icafast/output/ica_spikeF_list_", ncomp, ".rds")
    icalist <- readRDS(file = connect) 
    
    # create names for S, A and distance matrix (nrep_ncomp)
    reps <- rep(1:100, times = 1, each = ncomp)
    comps <- rep(1:ncomp, times = 100)
    names <- paste(reps, comps, sep = "_")
    
    # S---------------------------------#
    # list of 100 matrices
    S <- list()
    for(i in 1:100) {
      S[[i]] <- icalist[[i]]$S
    } 
    # put them all in one matrix
    S_mat <- do.call(cbind, S)
    
    # orient them consistently (the heavier tail to be positive, usually)
    col.medians <- apply(S_mat, 2, median)
    sign <- ifelse(col.medians > 0, -1, 1)
    S_mat <- t(t(S_mat)*sign)
    colnames(S_mat) <- names
    # saveRDS(S_mat, file = fname) # no need if you extract all info now
    
    # get spearman distances for clustering
    Spear <- cor(S_mat, method = "spearman")
    Spear <- 1 - abs(Spear)
    colnames(Spear) <- names
    rownames(Spear) <- names
    fname <- paste0("/data/cephfs/punim0613/icafast/output/spikeF_Spear_", ncomp, ".rds")
    saveRDS(Spear, file = fname)
    
    # PAM clustering --- do I need to use eval(ncomp) here ?
    pam_out <- pam(x = Spear, k = eval(ncomp), diss = TRUE, medoids = NULL, stand = F, cluster.only = F, 
                   do.swap = TRUE, keep.diss = F, keep.data = F, pamonce = 5, trace.lev = 1)
    fname <- paste0("/data/cephfs/punim0613/icafast/output/spikeF_PAM_", ncomp, ".rds")
    saveRDS(pam_out, file = fname)
    
    # average silhouette stat
    silh_width_ave[j] <- pam_out$silinfo$avg.width 
    # per cluster (consensus component) silhouette stat
    silh_width_clust[[j]] <- pam_out$silinfo$clus.avg.widths 
    
    # extract S consensus (medoids) components and "diagonal" (first run) components
    medoids <- pam_out$medoids # by nrep_ncomp names
    S_mat_c <- S_mat[, medoids]
    S_mat_d <- S_mat[, 1:ncomp]
    
    rm(S, S_mat, Spear)
    
    # A---------------------------------#
    # list of 100 matrices
    A <- list()
    for(i in 1:100) {
      A[[i]] <- icalist[[i]]$M
    } 
    # put them all in one matrix
    A_mat <- do.call(cbind, A)
    
    # orient them same way as S
    A_mat <- t(t(A_mat)*sign)
    colnames(A_mat) <- names
    # saveRDS(A_mat, file = fname) # no need if you extract all info now
    
    # extract A consensus (medoids) components and "diagonal" (first run) components
    A_mat_c <- A_mat[, medoids]
    A_mat_d <- A_mat[, 1:ncomp]
    
    rm(A, A_mat)
    
    ### before saving consensus S & A matrices, 
    ### order them by "variance accounted for" and rename
    vaf.c <- colSums(A_mat_c^2)
    vaf.c.order <- sort(vaf.c, decreasing = TRUE, index.return = TRUE)$ix
    
    # consensus S & A
    S_mat_c <- S_mat_c[, vaf.c.order]
    colnames(S_mat_c) <- as.character(1:ncomp)
    S_c[[j]] <- S_mat_c
    
    A_mat_c <- A_mat_c[, vaf.c.order]
    colnames(A_mat_c) <- as.character(1:ncomp)
    A_c[[j]] <- A_mat_c
    
    # "diagonal" S & A
    colnames(S_mat_d) <- as.character(1:ncomp)
    S_d[[j]] <- S_mat_d
    
    colnames(A_mat_d) <- as.character(1:ncomp)
    A_d[[j]] <- A_mat_d
    
    
    # Frobenius (rel.)------------------#
    
    # consensus
    E_c <- X - tcrossprod(S_mat_c, A_mat_c) # reconstitution error
    F_c2 <- (norm(E_c, type = "F"))^2 # squared frobenius norm of error 
    norm_F_c <- F_c2 / F_X2 # normalised frobenius norm of reconstitution error (1-fit)
    
    # diagonal
    E_d <- X - tcrossprod(S_mat_d, A_mat_d)
    F_d2 <- (norm(E_d, type = "F"))^2 
    norm_F_d <- F_d2 / F_X2
    
    norm_F_error_c[j] <- norm_F_c
    norm_F_error_d[j] <- norm_F_d
    
    
    # vafs------------------------------#
    
    # consensus ordered vafs need to be recalculated from consensus A mat
    vaf_c[[j]] <- (vaf.c[vaf.c.order] * ngenes)/F_X2
    
    # diagonal ordered vafs can be extracted directly from first ica run
    vaf_d[[j]] <- icalist[[1]]$vafs
    
    # null vafs (read in from null_icafast_all output)
    connect <- paste0("/data/cephfs/punim0613/icafast/output/null_ica_spikeF_", ncomp, ".rds")
    null.vaf <- readRDS(file = connect)
    null_vaf[[j]] <- apply(null.vaf, 1, mean) # mean of 10 runs, if SE & CI needed do it in R session
    
    rm(icalist)
    print(paste(ncomp, "done...")); print(Sys.time()); cat("\n")
    
    
  }
  
  # save S & A consensus & "diagonal" components (each is a list of five matrices)
  # consensus
  names(S_c) <- as.character(c(50,100,150,200,250))
  saveRDS(S_c, file = "/data/cephfs/punim0613/icafast/output/spikeF_S_c.rds")
  names(A_c) <- as.character(c(50,100,150,200,250))
  saveRDS(A_c, file = "/data/cephfs/punim0613/icafast/output/spikeF_A_c.rds")
  # diagonal
  names(S_d) <- as.character(c(50,100,150,200,250))
  saveRDS(S_d, file = "/data/cephfs/punim0613/icafast/output/spikeF_S_d.rds")
  names(A_d) <- as.character(c(50,100,150,200,250))
  saveRDS(A_d, file = "/data/cephfs/punim0613/icafast/output/spikeF_A_d.rds")
  
  # save reconstitution errors (list of two vectors - consensus & diagonal - each of length 5)
  reconst_errors <- list(norm_F_error_c, norm_F_error_d)
  names(reconst_errors) <- c('norm_F_error_c', 'norm_F_error_d')
  saveRDS(reconst_errors, file = "/data/cephfs/punim0613/icafast/output/spikeF_reconst_e.rds")
  
  # save silhouette stats for consensus components (ave is vecotr of length 5; clust is list of 5 vectors, each of length 50-250)
  silh_stats <- list(silh_width_ave, silh_width_clust)
  saveRDS(silh_stats, file = "/data/cephfs/punim0613/icafast/output/spikeF_silh_stats.rds")
  
  # save vafs (list of three lists - consensus, diagonal and null - each a list of five vectors of length 50-250)
  vafs <- list(vaf_c, vaf_d, null_vaf)
  names(vafs) <- c('vaf_c', 'vaf_d', 'null_vaf')
  saveRDS(vafs, file = "/data/cephfs/punim0613/icafast/output/spikeF_vafs.rds")
  
  # transfer these outputs from cluster to workfolder.. work can continue in R session (see chose_ncomp.R)
  # (no need to transfer Spearman and PAM, but keep them temporarily as they are computationally demanding steps)
  
}


#-------------------------------------------------------------------------------------------------------------------------------#
if(data.version == "sumF"){
  
  print("data version: sumF"); cat("\n")
  
  silh_width_ave <- rep(0,5)
  silh_width_clust <- list()
  
  S_c <- list()
  A_c <- list()
  
  S_d <- list()
  A_d <- list()
  
  norm_F_error_c <- rep(0,5)
  norm_F_error_d <- rep(0,5)
  
  vaf_c <- list()
  vaf_d <- list()
  null_vaf <- list()
  
  # read-in the original data matrix which was input for ica
  X <- readRDS(file = "/data/cephfs/punim0613/icafast/data/tm.smartseq2.sumF.mat.hvg.123.rds")
  X <- as.matrix(X)
  X <- scale(X, scale = FALSE)
  # prep this to calculate relative Frobenius errors & "variance accounted for" below
  F_X2 <- (norm(X, type = "F"))^2 # squared Frobenius norm of data matrix
  ngenes <- nrow(X)
  
  # read-in fastica output lists for each ncomp
  for(ncomp in c(50,100,150,200,250)) {
    
    
    j <- ncomp / 50
    
    connect <- paste0("/data/cephfs/punim0613/icafast/output/ica_sumF_list_", ncomp, ".rds")
    icalist <- readRDS(file = connect) 
    
    # create names for S, A and distance matrix (nrep_ncomp)
    reps <- rep(1:100, times = 1, each = ncomp)
    comps <- rep(1:ncomp, times = 100)
    names <- paste(reps, comps, sep = "_")
    
    # S---------------------------------#
    # list of 100 matrices
    S <- list()
    for(i in 1:100) {
      S[[i]] <- icalist[[i]]$S
    } 
    # put them all in one matrix
    S_mat <- do.call(cbind, S)
    
    # orient them consistently (the heavier tail to be positive, usually)
    col.medians <- apply(S_mat, 2, median)
    sign <- ifelse(col.medians > 0, -1, 1)
    S_mat <- t(t(S_mat)*sign)
    colnames(S_mat) <- names
    # saveRDS(S_mat, file = fname) # no need if you extract all info now
    
    # get spearman distances for clustering
    Spear <- cor(S_mat, method = "spearman")
    Spear <- 1 - abs(Spear)
    colnames(Spear) <- names
    rownames(Spear) <- names
    fname <- paste0("/data/cephfs/punim0613/icafast/output/sumF_Spear_", ncomp, ".rds")
    saveRDS(Spear, file = fname)
    
    # PAM clustering --- do I need to use eval(ncomp) here ?
    pam_out <- pam(x = Spear, k = eval(ncomp), diss = TRUE, medoids = NULL, stand = F, cluster.only = F, 
                   do.swap = TRUE, keep.diss = F, keep.data = F, pamonce = 5, trace.lev = 1)
    fname <- paste0("/data/cephfs/punim0613/icafast/output/sumF_PAM_", ncomp, ".rds")
    saveRDS(pam_out, file = fname)
    
    # average silhouette stat
    silh_width_ave[j] <- pam_out$silinfo$avg.width 
    # per cluster (consensus component) silhouette stat
    silh_width_clust[[j]] <- pam_out$silinfo$clus.avg.widths 
    
    # extract S consensus (medoids) components and "diagonal" (first run) components
    medoids <- pam_out$medoids # by nrep_ncomp names
    S_mat_c <- S_mat[, medoids]
    S_mat_d <- S_mat[, 1:ncomp]
    
    rm(S, S_mat, Spear)
    
    # A---------------------------------#
    # list of 100 matrices
    A <- list()
    for(i in 1:100) {
      A[[i]] <- icalist[[i]]$M
    } 
    # put them all in one matrix
    A_mat <- do.call(cbind, A)
    
    # orient them same way as S
    A_mat <- t(t(A_mat)*sign)
    colnames(A_mat) <- names
    # saveRDS(A_mat, file = fname) # no need if you extract all info now
    
    # extract A consensus (medoids) components and "diagonal" (first run) components
    A_mat_c <- A_mat[, medoids]
    A_mat_d <- A_mat[, 1:ncomp]
    
    rm(A, A_mat)
    
    ### before saving consensus S & A matrices, 
    ### order them by "variance accounted for" and rename
    vaf.c <- colSums(A_mat_c^2)
    vaf.c.order <- sort(vaf.c, decreasing = TRUE, index.return = TRUE)$ix
    
    # consensus S & A
    S_mat_c <- S_mat_c[, vaf.c.order]
    colnames(S_mat_c) <- as.character(1:ncomp)
    S_c[[j]] <- S_mat_c
    
    A_mat_c <- A_mat_c[, vaf.c.order]
    colnames(A_mat_c) <- as.character(1:ncomp)
    A_c[[j]] <- A_mat_c
    
    # "diagonal" S & A
    colnames(S_mat_d) <- as.character(1:ncomp)
    S_d[[j]] <- S_mat_d
    
    colnames(A_mat_d) <- as.character(1:ncomp)
    A_d[[j]] <- A_mat_d
    
    
    # Frobenius (rel.)------------------#
    
    # consensus
    E_c <- X - tcrossprod(S_mat_c, A_mat_c) # reconstitution error
    F_c2 <- (norm(E_c, type = "F"))^2 # squared frobenius norm of error 
    norm_F_c <- F_c2 / F_X2 # normalised frobenius norm of reconstitution error (1-fit)
    
    # diagonal
    E_d <- X - tcrossprod(S_mat_d, A_mat_d)
    F_d2 <- (norm(E_d, type = "F"))^2 
    norm_F_d <- F_d2 / F_X2
    
    norm_F_error_c[j] <- norm_F_c
    norm_F_error_d[j] <- norm_F_d
    
    
    # vafs------------------------------#
    
    # consensus ordered vafs need to be recalculated from consensus A mat
    vaf_c[[j]] <- (vaf.c[vaf.c.order] * ngenes)/F_X2
    
    # diagonal ordered vafs can be extracted directly from first ica run
    vaf_d[[j]] <- icalist[[1]]$vafs
    
    # null vafs (read in from null_icafast_all output)
    connect <- paste0("/data/cephfs/punim0613/icafast/output/null_ica_sumF_", ncomp, ".rds")
    null.vaf <- readRDS(file = connect)
    null_vaf[[j]] <- apply(null.vaf, 1, mean) # mean of 10 runs, if SE & CI needed do it in R session
    
    rm(icalist)
    print(paste(ncomp, "done...")); print(Sys.time()); cat("\n")
    
    
  }
  
  # save S & A consensus & "diagonal" components (each is a list of five matrices)
  # consensus
  names(S_c) <- as.character(c(50,100,150,200,250))
  saveRDS(S_c, file = "/data/cephfs/punim0613/icafast/output/sumF_S_c.rds")
  names(A_c) <- as.character(c(50,100,150,200,250))
  saveRDS(A_c, file = "/data/cephfs/punim0613/icafast/output/sumF_A_c.rds")
  # diagonal
  names(S_d) <- as.character(c(50,100,150,200,250))
  saveRDS(S_d, file = "/data/cephfs/punim0613/icafast/output/sumF_S_d.rds")
  names(A_d) <- as.character(c(50,100,150,200,250))
  saveRDS(A_d, file = "/data/cephfs/punim0613/icafast/output/sumF_A_d.rds")
  
  # save reconstitution errors (list of two vectors - consensus & diagonal - each of length 5)
  reconst_errors <- list(norm_F_error_c, norm_F_error_d)
  names(reconst_errors) <- c('norm_F_error_c', 'norm_F_error_d')
  saveRDS(reconst_errors, file = "/data/cephfs/punim0613/icafast/output/sumF_reconst_e.rds")
  
  # save silhouette stats for consensus components (ave is vecotr of length 5; clust is list of 5 vectors, each of length 50-250)
  silh_stats <- list(silh_width_ave, silh_width_clust)
  saveRDS(silh_stats, file = "/data/cephfs/punim0613/icafast/output/sumF_silh_stats.rds")
  
  # save vafs (list of three lists - consensus, diagonal and null - each a list of five vectors of length 50-250)
  vafs <- list(vaf_c, vaf_d, null_vaf)
  names(vafs) <- c('vaf_c', 'vaf_d', 'null_vaf')
  saveRDS(vafs, file = "/data/cephfs/punim0613/icafast/output/sumF_vafs.rds")
  
  # transfer these outputs from cluster to workfolder.. work can continue in R session (see chose_ncomp.R)
  # (no need to transfer Spearman and PAM, but keep them temporarily as they are computationally demanding steps)
  
}


